defaults:
  do_train: true
  do_eval: false
#  base_model: "princeton-nlp/Sheared-LLaMA-1.3B"
  base_model: "deepseek-ai/deepseek-coder-1.3b-base"
  total_batch_size: 32 # total batch size # TODO rename later
  per_device_eval_batch_size: 2
  per_device_train_batch_size: 2
  warmup_steps: 5000
  save_steps: 900
  num_train_epochs: 1
  logging_steps: 1
  add_special_tokens: false
  use_fast_tokenizer: false
  node: "localhost"
  accumulate_summary: true
  remove_unused_columns: false
  train_domains:
    - "Books3"
    - "Github"
    - "FreeLaw"
    - "Wikipedia"
  eval_domains:
    - "Books3"
    - "Github"
    - "FreeLaw"
    - "Wikipedia"
    - "Gutenberg"
    - "HackerNews"
    - "ArXiv"
    - "YoutubeSubtitles"

data:
#  preprocessed_train_datasets: ["awettig/RedPajama-combined-15B-6K-llama"]
  preprocessed_train_datasets: ["/mnt/data2/shared-data/autocompressors/6k_py_135664_samp"]
  preprocessing_num_workers: 0
  dataloader_num_workers: 0
  max_train_samples: null
  streaming_data: false
  upload_aws: false
  s3_bucket: "jettrain-experiments"
  s3_prefix: "Mikhail.Arkhipov/autocompressors/checkpoints/"
  s3_cred_filepath: "configs/aws_credentials"

output:
  log_level: critical
  disable_tqdm: true
  out_dir: "/mnt/data2/arkhipov/experiments/autocompressors/"

training:
  summary_accumulation: true
  lora: true
  train_embed_only: false
  use_kv: false
  segments_per_substep: 2
  training_substeps: 3
  summary_length: 50
  randomize_substeps: true
  randomize_std: 0.2
  segment_gradient_checkpointing: false
  learning_rate: 8e-4
  resume_from_checkpoint: false
  # checkpoint_path: "/mnt/data2/galimzyanov/autocompressor/checkpoints/LLaMA-1.3B_sub3_seg2_sum50_embed_only_test/checkpoint-25"
  checkpointing: true  # If your training script supports checkpointing
  bf16: true
  use_accelerate: true

lora:
  lora_r: 16
  lora_alpha: 16
  lora_dropout: 0.05
  lora_target_modules: ["q_proj", "v_proj", "o_proj", "k_proj"]
  lora_modules_to_save: ["embed_summary"]

wandb:
  report_to: "wandb"
  run_name: "deepseek-1.3B"
  dir: "checkpoints/" #"/mnt/data2/galimzyanov/autocompressor/checkpoints/"
  project: "autocompressors"
#  resume_run: true