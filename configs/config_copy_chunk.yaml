defaults:
  do_train: true
  do_eval: false
  # base_model: "princeton-nlp/Sheared-LLaMA-1.3B"
  base_model: "deepseek-ai/deepseek-coder-1.3b-base"
  total_batch_size: 32 # total batch size # TODO rename later
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  eval_samples: 300
  warmup_steps: 500
  save_steps: 1000
  num_train_epochs: 10
  logging_steps: 1
  add_special_tokens: false
  use_fast_tokenizer: false
  node: "localhost"
  accumulate_summary: true
  remove_unused_columns: false
  train_domains:
    - "Books3"
    - "Github"
    - "FreeLaw"
    - "Wikipedia"
  eval_domains:
    - "Books3"
    - "Github"
    - "FreeLaw"
    - "Wikipedia"
    - "Gutenberg"
    - "HackerNews"
    - "ArXiv"
    - "YoutubeSubtitles"

data:
  # preprocessed_train_datasets: ["awettig/RedPajama-combined-15B-6K-llama"]
  # preprocessed_validation_datasets: ["awettig/RedPajama-combined-15B-6K-llama"]
  preprocessed_train_datasets: ["/mnt/data2/shared-data/autocompressors/6k_py_320000_samp/train"]
  preprocessed_validation_datasets: ["/mnt/data2/shared-data/autocompressors/6k_py_320000_samp/valid"]
  # max_context_length: 2048
  preprocessing_num_workers: 0
  dataloader_num_workers: 0
  max_train_samples: null
  # cache_dir: "/mnt/data2/galimzyanov/autocompressor/temp/"
  streaming_data: false
  upload_aws: false
  s3_bucket: "jettrain-experiments"
  s3_prefix: "Timur.Galimzyanov/outputs/autocompressors/checkpoints/"
  s3_cred_filepath: "configs/aws_credentials"

output:
  log_level: critical
  disable_tqdm: true
  out_dir: "/mnt/data2/galimzyanov/autocompressor/checkpoints/"

training:
  summary_accumulation: true
  lora: true
  train_embed_only: false
  use_kv: false
  segments_per_substep: 2 # 2 # 6 # 1
  training_substeps: 3 # 3 # 4 # 1
  summary_length: 50 # 50 # 12 # 0
  copy_last_chunk: True
  only_last_chunk_loss: True
  randomize_substeps: true
  randomize_std: 0.2 # 0.2 # 0
  segment_gradient_checkpointing: false
  learning_rate: 8e-4
  resume_from_checkpoint: false
  # checkpoint_path: "/mnt/data2/galimzyanov/autocompressor/checkpoints/LLaMA-1.3B_sub3_seg2_sum50_shuffle_old/checkpoint-2100"
  checkpointing: true  # If your training script supports checkpointing
  bf16: true
  use_accelerate: true

lora:
  lora_r: 16
  lora_alpha: 16
  lora_dropout: 0.05
  lora_target_modules: ["q_proj", "v_proj", "o_proj", "k_proj"]
  lora_modules_to_save: ["embed_summary"]

wandb:
  report_to: "wandb"
  run_name: "deepseek-1.3B"
  # run_name: "LLaMA-1.3B"
  dir: "/mnt/data2/galimzyanov/autocompressor/checkpoints/"
  project: "autocompressors"
  resume_run: false
