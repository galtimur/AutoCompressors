defaults:
  do_train: true
  do_eval: false
  base_model: "princeton-nlp/Sheared-LLaMA-1.3B"
  total: 30 # total batch size # TODO rename later
  per_device_eval_batch_size: 2
  per_device_train_batch_size: 2
  warmup_steps: 5000
  save_steps: 300
  num_train_epochs: 1
  logging_steps: 1
  add_special_tokens: false
  use_fast_tokenizer: false
  node: "localhost"
  accumulate_summary: true
  remove_unused_columns: false
  train_domains:
    - "Books3"
    - "Github"
    - "FreeLaw"
    - "Wikipedia"
  eval_domains:
    - "Books3"
    - "Github"
    - "FreeLaw"
    - "Wikipedia"
    - "Gutenberg"
    - "HackerNews"
    - "ArXiv"
    - "YoutubeSubtitles"

data:
  preprocessed_train_datasets: ["awettig/RedPajama-combined-15B-6K-llama"]
  preprocessing_num_workers: 0
  dataloader_num_workers: 0
  max_train_samples: null
  # cache_dir: "/mnt/data2/galimzyanov/autocompressor/temp/"
  streaming_data: fasle

output:
  log_level: critical
  disable_tqdm: true
  out_dir: "/mnt/data2/galimzyanov/autocompressor/checkpoints/"

training:
  summary_accumulation: true
  lora: false
  train_embed_only: true
  use_kv: true
  segments_per_substep: 2
  training_substeps: 3
  summary_length: 50
  randomize_substeps: true
  randomize_std: 0.2
  segment_gradient_checkpointing: false
  learning_rate: 4e-4
  resume_from_checkpoint: false
  # checkpoint_path: "/mnt/data2/galimzyanov/autocompressor/checkpoints/LLaMA-1.3B_sub3_seg2_sum50_embed_only_test/checkpoint-25"
  checkpointing: true  # If your training script supports checkpointing
  bf16: true
  use_accelerate: true

lora:
  lora_r: 16
  lora_alpha: 16
  lora_dropout: 0.05
  lora_target_modules: ["q_proj", "v_proj", "o_proj", "k_proj"]
  lora_modules_to_save: ["embed_summary"]

wandb:
  report_to: "wandb"
  run_name: "LLaMA-1.3B"
  dir: "/mnt/data2/galimzyanov/autocompressor/checkpoints/"
  project: "autocompressors"
#  resume_run: true
